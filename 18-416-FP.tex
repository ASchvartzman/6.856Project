%Jennifer Pan, August 2011

\documentclass[11pt,letter]{article}
	% basic article document class
	% use percent signs to make comments to yourself -- they will not show up.
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{complexity}
\usepackage{amssymb}
\usepackage{amsthm}
% This gave Ziv a compile error: \usepackage[inputenc]{utf8}
\usepackage[charter]{mathdesign} % Ziv likes this font....
\usepackage[english]{babel}

	% packages that allow mathematical formatting

\usepackage{graphicx}
	% package that allowxs you to include graphics

\usepackage{setspace}
	% package that allows you to change spacing

\usepackage{fullpage}
	% package that specifies normal margins

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\numberwithin{theorem}{section}

% Make life easier and use nicer looking greek letters, etc.
\newcommand{\ol}{\overline}
\renewcommand{\phi}{\varphi}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\emptyset}{\varnothing}


\begin{document}
	% line of code telling latex that your document is beginning


\title{On some recent purely combinatorial 3/4-approximation algorithms for the Maximum Satisfiability Problem}

\author{Jonathan Elzur, Ariel Schvartzman, Ziv Scully \\
  \{\texttt{jelzur}, \texttt{arielsc}, \texttt{ziv}\}\texttt{@mit.edu}}
% Note: when you omit this command, the current dateis automatically included

\maketitle
	% tells latex to follow your header (e.g., title, author) commands.

INSERT ABSTRACT?

\section{Introduction}

In the Maximum Satisfiability Problem (MAX-SAT) we are given a set of boolean variables $x_1, ..., x_n$
and clauses $C_1,...,C_m$ involving them and want an assignment of the variables that satisfies the most clauses.
In class we analyzed the unweighted case, but here we consider non-negative weights $w_i$ for the clauses
and ask for the weight of the satisfied clauses to be maximized. This problem, which is a generalization of the
Boolean Satisfiability Problem (SAT), is also $\NP$-hard.

This problem has been studied extensively. Let $W = \sum w_i$ be the sum of the clause weights.
A simple randomized algorithm sets each variable to true with probability $1/2$ and hence,
by linearity of expectation, satisfies $(1/2)W$ of the clauses. Johnson \cite{Johnson1974256}
gave the first deterministic algorithm to match this ratio. Consider rescaling the weight of a clause $C_j$
by its length in the following way: $f(C_j) = w_j 2^{-|C_j|}$. This weight function favors small clauses
since they are easier to falsify. Johnson's algorithm sets the $i$-th variable to be true
if the modified weight of the clauses containing $x_i$ is greater than that of the clauses containing its negation.
It was later shown that Johnson's algorithm was in fact the derandomization of the first algorithm and that the
approximation ratio of the derandomized algorithm could be improved to $2/3$ \cite{Chen1999622}.

Later, Yanakakis \cite{Yannakakis1994475} and separately Goemans and Williamson \cite{Goemans94new3/4-approximation}
gave algorithms which achieved $3/4$-approximation ratios. In fact, we studied one of these in class.
These algorithms relied heavily on linear programming relaxations. The best known approximation algorithm due to Avidor, Berkovitch
and Zwick achieves an approximation ratio of 0.797. The algorithm is a hybrid which runs multiple MAX SAT approximation algorithms
(including LP based ones) and returns the best. [cite] They also present an algorithm which is conjectured to have a performance of 0.843. 

In 1999, Williamson posed the question
of whether or not the same approximation ratio could be achieved without using Linear Programming. In 2011,
Poloczek and Schnitger \cite{Poloczek:2011:RVJ:2133036.2133087} answered this question in the affirmative
by exhibiting the first purely combinatorial algorithm for MAX-SAT. Their approach is similar to that of Johnson
but they introduce slight weight modifications that guarantee that at least $3/4$ of the weighted clauses will be satisfied.
While this result was exciting, the probability modifications are complicated and depend on the previous decisions
of the algorithm. Shortly after, van Zuylen \cite{vanZuylen:2011:SAM:2238496.2238512} gave a simpler analysis of the algorithm.
Both algorithms use randomness but are greedy in some sense,
setting a variable true or false with probability based on
how many clauses would be satisfied either way.

This paper is organized as follows.
In Section \ref{S:idea}, we present some notation and highlight the common techniques of both papers
and show how they could provide a $3/4$-approximation.
In Section \ref{S:PS}, we present the main ideas of Poloczek and Schtinger's paper.
In Section \ref{S:vZ}, we present van Zuylen's simplified approach.
In Section \ref{S:limits}, we present results on the limits of
the greedy approach the two algorithms share,
showing that the previous algorithms are in some sense optimal
and, more strikingly,
that no better than a $0.729$-approxmation is possible without randomness.


\section{The General Idea}\label{S:idea}

Let the input variables be $x_1,...,x_n$ and the input clauses be $C_1,...,C_m$
each with associated non-negative weights $w_1,...,w_m$. Let $\sum_{i=1}^{m} w_i = W$.
We are interested in finding an assignment of the variables $x_i$ such that the clauses satisfied have weight at least $\frac{3}{4}W$.

Our algorithm will be sequentially deciding what value to assign to each variable. Let $\mathrm{SAT}(i)$ be the total weight of the clauses
satisfied by our partial assignment $x_1, ..., x_i$ and let $\mathrm{UNSAT}(i)$ be the total weight of the clauses that are falsified (i.e. unsatisfiable) by the current partial assignment.
Suppose we have set variables $x_1,...,x_{i-1}$ and are considering a value for variable $x_i$. Then $\mathrm{SAT}(i)-\mathrm{SAT}(i-1)$ is the weight
of the clauses that become satisfied by assigning $x_i$ and similarly $\mathrm{UNSAT}(i) - \mathrm{UNSAT}(i-1)$ is the weight of the clauses that become
unsatisfiable by assigning $x_i$.
Notice that $\mathrm{SAT}(n)$ is the total weight of the clauses satisfied by our algorithm and $\mathrm{UNSAT}(n) = W - \mathrm{SAT}(n)$ is the weight of clauses
unsatisfied by the algorithm. We define sat($i$) = SAT($i$) - SAT($i-1$), unsat($i$) = UNSAT($i$) - UNSAT($i-1$). Both algorithms we study rely on the following fact to be true to meet their guarantees:

\begin{equation}
\label{eq:1}
\mathrm{sat}(i) - 3\mathrm{unsat}(i) \geq 0 \forall i.
\end{equation}

Adding these inequalities over all $i$ we get that

\begin{equation*}
\begin{aligned}
\sum_{i = 1}^{m} \mathrm{sat}(i) - 3\mathrm{unsat}(i) & = \mathrm{SAT}(n) - 3\mathrm{UNSAT}(n) \\
& = 4\mathrm{SAT}(n) - 3W \\
& \geq 0
\end{aligned}
\end{equation*}

From this we get that our algorithms approximation ratio, $\textrm{SAT}(n)$, is at least $\frac{3}{4}$ of the maximum possible weight $W$.

There might not always exist an assignment for $i$ such that the equation (cite) above holds. However, we only need it to hold on aggregate.
This is why the notion of a potential function $\phi$ is introduced. Throughout the algorithm, we will use the potential function to help us pay
for future mistakes where the inequality above might not hold.

Van Zuylen explains the desired behavior of this potential function in terms of other parameters of the problem. If these conditions are met, the algorithm
will achieve its guarantees. Let $\phi(i)$ be the value of the potential function after assigning the $i$-th variable. Then we need $\phi$ to satisfy the following properties:

\begin{itemize}
	\item $\phi(0) \leq 3(W-\textrm{OPT})$
	\item $\phi(n) \geq 0$
	\item For each $x_i$, the algorithm randomly determines a truth assignment to $x_i$ such that
\begin{equation}
\begin{aligned}
\label{eq:2}
\mathbb{E}[\mathrm{sat}(i) - 3\mathrm{unsat}(i)] \\
\geq \mathbb{E}(\phi(i) - \phi(i-1)].
\end{aligned}
\end{equation}
\end{itemize}

If we have such a potential function, then $\mathbb{E}[\textrm{SAT}(n)- 3\left(W-\textrm{SAT}(n)\right)] \geq \phi(n) - \phi(0) \geq 3(W-\textrm{OPT})$.
This directly implies $\mathbb{E}[\textrm{SAT}(n)] \geq \frac{3}{4} W \geq \frac{3}{4} \textrm{OPT}$.

We utilize the rest of this section to introduce some notation that will be useful for sections 3 and 4.
Let $x_i^*$ be the optimal assignment of $x_i$ and $x_i^{a}$ be the algorithm's assignment. We say a clause is alive
at time $i$ if it contains some literal $x_j$ with $j > i$ and it is not yet satisfied by $x_1^a,...,x_i^a$.
We will say a clause is contradictory at time $i$ if it is not satisfied by $x_1^a,...,x_i^a,x_{i+1}^*,...,x_n^*$.


\section{Poloczek and Schtinger's approach}\label{S:PS}

Here we present Poloczek and Shcnitger's algorithm. Their approach is based on randomizing Johnson's algorithm.

\subsection*{Johnson's Deterministic Algorithm}

Consider, as mentioned above, the scaled clause weight. For a clause $C_j$, with weight $w_j$, we have the modified weight:  $f(C_j) = w_j 2^{-|C_j|}$.

Johnson's Algorithm processes the variables in an arbitrary order, and assigns them a value of 0 or 1 as follows. Let $x$ be the variable being assigned a value. We define the \emph{support} of $x$, $\mu_x$ to be the sum of the scaled weights of clauses containing $x$. Similarly, $\mu_{\bar{x}}$ is the sum of scaled weights of clauses containing $\bar{x}$:

\begin{align*}
\mu_x = \sum_{C,x\in C}f(C), \,\,\,\, \mu_{\bar{x}} =\sum_{C,\bar{x}\in C}f(C)
\end{align*}
Johnson's Algorithm greedily assigns $x=1$ if and only if $\mu_x \ge \mu_{\bar{x}}$.

\subsection*{The Canonical Randomization}
Poloczek and Schnitger consider the \emph{Canonical Randomization} (CR) of Johnson's Algorithm. Instead of deterministically assigning a value to the variable $x$, we assign it a value of 1 with probability proportional to the support of $x$ relative to that of $\bar{x}$. So
\[\Pr[\mathrm{assign } x=1] = \frac{\mu_x}{\mu_x + \mu_{\bar{x}}},\] and $x$ equals zero with the remaining probability. They prove the limits of this approach. We state their result without proof.

\begin{lemma}
\label{L:1}
The approximation ratio of CR algorithm is upper-bounded by:
$$\frac{17}{23} < \frac{3}{4}$$

\end{lemma}

\subsection*{Alternative Probabilities}
Due to Lemma \ref{L:1}, Poloczek and Schnitger decide to use an alternative weighting scheme. In this alternative, every clause with a single variable gets its weight doubled, while the remaining weights are unchanged. Therefore, the new support for $x$ which we can label $\lambda_x$ is:
\[[\textrm{sum of weights of clauses with }x\textrm{ with }\ge 2\textrm{ variables}] + 2[\textrm{sum of weights of clause containing just }x].\]

If we call the first sum $\mathsf{fanin}$, and label the second sum $w_x$, then:
\begin{eqnarray*}
\lambda_x \equiv \textrm{Support for } x = \mathsf{fanin} + 2w_x\\
\lambda_{\bar{x}} \equiv \textrm{Support for } \bar{x} = \mathsf{fanout} + 2w_{\bar{x}}
\end{eqnarray*}
Finally, we define $\Delta$ to be the sum of the supports, so $\Delta = \lambda_{x} + \lambda_{\bar{x}} = \mathsf{fanin} + 2w_x + \mathsf{fanout} + 2w_{\bar{x}}$. If we do a canonical randomization  using the new weight functions, we see that
\begin{eqnarray*}
q_0 &:=& \Pr[x=0] = \frac{\mathsf{fanout} + 2w_{\bar{x}}}{\Delta}\\
q_1 &:=& \Pr[x=1] = \frac{\mathsf{fanin} + 2w_x}{\Delta}
\end{eqnarray*}
Note that for a $2-\text{CNF}$ formula, these alternative weights are exactly equivalent to the weights used in Johnson's Algorithm. Thus, these modified weights still don't guarantee a $\frac{3}{4}$ approximation ratio.

\subsection*{Slack Algorithm}
The general approach of this algorithm is to change the probabilities, $q_0, q_1$ with which we assign values to $x$. In a sense, we're making the algorithm "less random" by increasing the higher of the two probabilities, bringing it closer to 1, and decreasing the lower of the two probabilities.

Assume w.l.o.g that $q_0 \le q_1$, so $x$ is at least as likely to assigned a value of 1. We now define the modified probabilities: $p_1 = q_1 + \varepsilon$, $p_0 = q_0 - \varepsilon$. We will show that for a good choice of $\varepsilon$, these probabilities yield a $\frac{3}{4}$ approximation. Doing this requires proving a few lemmas which Poloczek and Schnitger do in their paper. In order to better show the logical progression, we will defer some of the proofs to the end.

For any variable $x$, we define the random variable $\mathsf{slack}$ to be the magnitude of the difference between the support of $x$ and that of $\bar{x}$:
\begin{equation}
\mathsf{slack} = |(\mathsf{fanout} + 2w_{\bar{x}}) -(\mathsf{fanin} + 2w_x)|
\end{equation}
Let $x$ be the $i^{th}$ variable fixed. Following our notation in the introduction, let SAT($i$), UNSAT($i$) be random variables that denote the weight of all clauses that are satisfied, and unsatisfiable once we fix $x$, respectively. It follows that
\begin{eqnarray}
E[\textrm{sat}(i)] &=& p_0(\mathsf{fanout} + 2w_{\bar{x}}) + p_1(\mathsf{fanin} + 2w_{x})\\
E[\textrm{unsat}(i)] &=& p_0w_x + p_1w_{\bar{x}}
\end{eqnarray}
Now, we call a clause, $C$ containing $x$ or its negation \textit{alive} iff after assigning a value to $x$, $C$ still has not been satisfied or falsified. Further, we define the random variable ``wounded" to be the sum of the weights of all alive clauses. Therefore
\begin{equation}
E[\textrm{wounded}] = p_0 \cdot \mathsf{fanin} + p_1 \cdot \mathsf{fanout}
\end{equation}
Now Poloczek and Schnitger prove the following Lemma:
\begin{lemma} \label{L:2}
$E[\textrm{sat}(i) - 3\cdot\textrm{unsat}(i)] = E[\textrm{wounded} + \mathsf{slack}] - (w_x + w_{\bar{x}})$
\end{lemma}
Now consider some optimal assignment, $\pi$. Let us define $c(x)$ to be the weight of alive clauses that are not satisfied by $\pi$ before assigning a value to $x$, and $c'(x)$ be that same quantity after we assign a value to $x$. By linearity of expectations, we can add the quantity $2(c-c')$ to both sides of the equation of lemma \ref{L:2}, yielding:
\begin{equation}
E[\textrm{sat}(i) - 3\cdot\textrm{unsat}(i)+ 2(c-c')] = E[\textrm{wounded} + \mathsf{slack} + 2(c-c')] - (w_x + w_{\bar{x}})
\end{equation}
If we can manage to find $p_0,p_1$ so that the right side of the equation is always positive, then we have
\begin{equation}
E[\textrm{sat}(i) - 3\cdot\textrm{unsat}(i)+ 2(c-c')] \ge 0 \label{C:1}
\end{equation}
This condition can be rewritten as:
\begin{equation}
E[(\textrm{SAT}(i) - \textrm{SAT}(i-1)) - 3\cdot(\textrm{UNSAT}(i) - \textrm{UNSAT}(i-1))] \ge  2(c'-c) \ge 0
\end{equation}
Which is exactly the condition we stated both algorithms satisfy. Adding this up over all variables yields a telescoping sum, which cancels to
\begin{eqnarray}
E[\textrm{SAT}(n)] - 3E[\textrm{UNSAT}(n)] - 2(W-\textrm{OPT}) \ge 0
\\4E[SAT] \ge W + 2\textrm{OPT} \ge 3\textrm{OPT}
\end{eqnarray}
So the algorithm will have a $\frac{3}{4}$ approximation ratio if \ref{C:1} is satisfied. It is worth noting at this point that the term $\mathbb{E}(c'-c)$ is equal to
$\mathbb{E}(\phi(i) - \phi(i-1))$ mentioned in the previous section. To achieve the approximation ratio, the next lemma is useful:
\begin{lemma} \label{L:3}
Assume our assignment probabilities for $x$ are $\Pr[x = 1] = p_1, \Pr[x=0] = p_0$. If the optimal assignment is $x=1$, then:
\[E[\textrm{wounded} - 2(c'-c)] - (w_x - w_{\bar{x}}) \ge \frac{\mathsf{slack}}{\Delta}(w_x - w_{\bar{x}}) + \varepsilon(\mathsf{fanin} + \mathsf{fanout})\]
If the optimal assignment is  $x=0$, then:
\[E[\textrm{wounded} - 2(c'-c)] - (w_x - w_{\bar{x}}) \ge \frac{\mathsf{slack}}{\Delta}(w_x - w_{\bar{x}}) - \varepsilon(\mathsf{fanin} + \mathsf{fanout})\]
\end{lemma}
The proof of this lemma involves quite a bit of algebra and is therefore omitted here. The consequence of this lemma, though, is that we can use it to find a value for $\varepsilon$ so that  \ref{C:1} is always satisfied.

We want to express this constraint in terms of slack. So we note that from our definition of slack:
\begin{eqnarray}
E[\mathsf{slack}] &=& (p_1-p_0)\cdot ((\mathsf{fanout} + 2w_{\bar{x}}) -(\mathsf{fanin} + 2w_x))
\\&=& (q_1-q_0+2\epsilon)(q_1-q_0)\cdot\Delta
\\&=& \Delta[(q_1-q_0)^2 + 2\varepsilon(q_1-q_0)]
\\&=& \frac{\mathsf{slack}^2}{\Delta} +2\varepsilon\cdot\mathsf{slack}
\end{eqnarray}
Plugging this into \ref{C:1}, and applying \ref{L:3}, we get that if $x=1$ is the optimal assignment,
\[E[\textrm{sat}(i) - 3\cdot\textrm{unsat}(i)+ 2(c-c')] \ge \frac{\mathsf{slack}^2}{\Delta} +2\varepsilon\cdot\mathsf{slack} - \frac{\mathsf{slack}}{\Delta}(w_x + w_{\bar{x}}) + \varepsilon(\mathsf{fanin} + \mathsf{fanout})\].
Setting the this expression equal to zero and solving for $\varepsilon$ yields
\begin{equation}
\varepsilon = \varepsilon_1 \equiv \frac{-\frac{\mathsf{slack}^2}{\Delta} + \frac{\mathsf{slack}}{\Delta}(w_x + w_{\bar{x}})}{2\mathsf{slack}+ \mathsf{fanout} + \mathsf{fanin}}
\end{equation}
Thus this value of $\varepsilon$ yields exactly zero for the expression we want to be nonnegative. It's easy to check this also works for the case when $x=0$ is an optimal assignment. So, for every variable, we calculate $q_1, q_0$, calculate $\varepsilon_1$, and use it to get $P_1, p_0$. Then we assign $x=1$ with probability $p_1$. As we've just shown, this yields $E[SAT] \ge \frac{3}{4}\textrm{OPT}$.


\section{van Zuyeln's Simplified Algorithm}\label{S:vZ}

In this section we present van Zuylen's combinatorial algorithm.

\begin{lemma}
Consider the algorithm that sequentially assigns variables $x_1,...,x_n$. Given the assignment of $x_1,...,x_{i-1}$,
 let $W_i, \overline{W_i}$ be the weight of the clauses that are not yet satisfied and contain $x_i, \overline{x_i}$ respectively,
 but do not contain $x_{i+1}, ..., x_n$. Let $F_i, \overline{F_i}$ be the total weight of the clauses that are not yet satisfied
 that contain $x_i, \overline{x_i}$ respectively. Let $\alpha = \frac{W_i + F_i - \overline{W_i}}{F_i + \overline{F_i}}$, and let $x_i$ be set to true with probability

\begin{displaymath}
  p = \left\{
     \begin{array}{lr}
       0 & : \alpha \leq 0\\
       \alpha & : \alpha \in (0,1) \\
       1 & : \alpha \geq 1.
     \end{array}
   \right.
\end{displaymath}
Then the expected weight of the clauses satisfied by the algorithm is at least $\frac{3}{4} \textrm{OPT}$.
\end{lemma}

\begin{proof}
We will present a potential function $\phi$ such that it meets the criterion presented in section \ref{S:idea}. Our potential function will always
 be at least twice the weight of the clauses that are alive and contradictory at time $i$. Note that at time $0$,
 $\phi(0) = 2(W-\textrm{OPT})$. It is clear that the first two conditions will be met, so we will focus on showing equation \ref{eq:2}.
 For this purpose we will lower bound the weight of the contradictory clauses at time $i-1$ that are not alive at time $i$
 and upper bound the weight of the clauses that become contradictory at time $i$.

Note that a clause that is contradictory at time $i-1$ is no longer contradictory at time $i$ when either it becomes satisfied
or it contains literals $x_j$, $j > i$. Therefore a lower bound for the weight of contradictory clauses alive at time $i-1$ and
 not alive at time $i$ by $W_i \mathbbm{1}_{x_i^* = 0} + \overline{W_i} \mathbbm{1}_{x_i^* = 1}$, where $\mathbbm{1}_A$ is the indicator variable for event $A$ being true.

Similarly, the only clauses that can become contradictory at time $i$ are those that are still alive at time $i-1$ and become unsatisfied
 by the asigning $x_i$ to contradict $x_i^*$. Therefore, we can upper bound the weight of clauses that become contradictory by
 $F_i \mathbbm{1}_{x_i^* = 1} (1-p) + \overline{F_i} \mathbbm{1}_{x_i^* = 0}p$.

Then we get an upper bound on the change in potential

\begin{equation}
\label{eq:3}
\phi(i) - \phi(i-1) \leq 2(-W_i + \overline{F_i} p) \mathbbm{1}_{x_i^* = 0} + 2(-\overline{W_i} + F_i(1-p))\mathbbm{1}_{x_i^* = 1}
\end{equation}

On the other hand

\begin{equation}
\begin{aligned}
\label{eq:4}
& \textrm{SAT}(i) - \textrm{SAT}(i-1) - 3(\textrm{UNSAT}(i) - \textrm{UNSAT}(i-1)) \\
& = p(W_i + F_i - 3\overline{W_i}) + (1-p)(\overline{W_i} + \overline{F_i} - 3W_i)
\end{aligned}
\end{equation}

We will show that $(\ref{eq:4}) \geq (\ref{eq:3})$ by showing an upper bound $B$ on the right hand side of (\ref{eq:3}) such that $(\ref{eq:4}) \geq B \geq (\ref{eq:3})$.

Consider the case $\alpha \leq 0$, i.e $W_i + F_i \leq \overline{W_i}$. Then

\begin{equation*}
\begin{aligned}
(\ref{eq:3}) & = -2W_i \mathbbm{1}_{x_i^* = 0} + 2(-\overline{W}_i + F_i) \mathbbm{1}_{x_i^* = 1} \\ 
& \leq  -2W_i \mathbbm{1}_{x_i^* = 0} - 2W_i \mathbbm{1}_{x_i^* = 0} = -2W_i.
\end{aligned}
\end{equation*}

Then the right hand side of (\ref{eq:4}) is at least $\overline{W_i} + \overline{F_i} - 3W_i + 2W_i =
\overline{W_i} + \overline{F_i} - W_i.$

But this quantity can't be negative, because if it were we would combine it with $W_i + F_i - \overline{W_i} \leq 0$ to obtain $F_i +
\overline{F_i} < 0$, a contradiction.

Consider now the case where $\alpha = 1$, i.e. $W_i + F_i -\overline{W_i} \geq \overline{F_i} + F_i$ or $W_i - \overline{F_i} \geq \overline{W_i}$.
We can notice that (\ref{eq:3}) $= 2(-W_i + \overline{F_i}) \leq -2\overline{W_i}$. On the other hand, the right hand side of  (\ref{eq:4})
is at least $W_i + F_i + 2\overline{W_i} -3\overline{W_i} = W_i + F_i - \overline{W_i}$ which is at least $0$ since otherwise we would get
that $F_i + \overline{F_i} < 0$, a contradiction.

We now proceed with the more general case. The analysis is similar to the previous cases and will be left as an exercise
for the reader to save space and prevent further cluttering.
\end{proof}


\section{Limits of the Greedy Approach}\label{S:limits}

Poloczek showed in \cite{DBLP:conf/esa/Poloczek11} not only that
the randomized algorithms of the previous two sections
give the best possible approximation, namely $3/4$,
of any algorithm using the same greedy approach,
but also that the randomness was necessary for that performance.
We spend this section explaining these results,
mimicking much of Poloczek's notation and terminology.

Our first order of business is to make ``greedy approach'' precise.
To do this, we imagine the input as being of a series of \emph{data items},
each of which corresponds to a variable $x$,
which must be assigned permanently before moving on to the next data item.
A data item for $x$ contains a list of the following information
for each undecided clause $c$ that $x$ appears in:
\begin{itemize}
\item
  the sign of $x$ in $c$ (whether $x$ or $\ol{x}$ is in $c$),
\item
  the weight of $c$,
\item
  a list of the other unassigned variables (without sign information) in $c$.
\end{itemize}
Additionally, identical clauses are merged by adding their weights.
Note that data items depend on what has been assigned so far.

An \emph{online greedy algorithm},
such as those from Sections \ref{S:PS} and \ref{S:vZ},
is given the data items in some adversarial order,
while an \emph{adaptive greedy algorithm}
can choose at each step the sort of data item it would like to see next.
(Specifically, at each step,
it gives an ordering $\prec$ on all possible data items
and is given $\prec$-minimal remaining data item.)
We know that online greedy algorithms can achieve $3/4$-approximations,
but we might hope that cleverer choice of weights
(e.g. $\alpha$ in van Zuyeln's algorithm)
could yield a better approximation.
Alas, no such cleverness is possible.

\begin{theorem}\label{theorem:greedyRandomized}
  No randomized online greedy algorithm
  is better than a $3/4$-approximation in expectation.
\end{theorem}

A small example (see \cite{DBLP:conf/esa/Poloczek11}) shows that
deterministic online greedy algorithms cannot do better than $2/3$,
and even even if we allow them to be adaptive, we still can't reach $3/4$.

\begin{theorem}\label{theorem:greedyDeterministic}
  No deterministic adaptive greedy algorithm
  is better than a $\frac{3 + \sqrt{33}}{12}$-approximation.
\end{theorem}

For the proofs of both of these theorems,
we will reason at a level of abstraction higher than clauses.
An \emph{equivalence} between two variables $x$ and $y$
is the pair of clauses $x \lor \ol{y}$ and $\ol{x} \lor y$.
Similarly, an \emph{inequivalence} is
the pair $x \lor y$ and $\ol{x} \lor \ol{y}$.
If all clauses have weight 1,
an equivalence contributes weight 2 when $x = y$
and only weight 1 when $x \neq y$,
while an inequivalence contributes weight 2 when $x \neq y$
and only weight 1 when $x = y$.
Abusing terminology slightly,
we say an (in)equivalence is satisfied
when it contributes weight 2 rather than only weight 1.

\begin{proof}[Proof of Theorem \ref{theorem:greedyRandomized}]
  The construction given here is original
  but uses the same idea as that in \cite{DBLP:conf/esa/Poloczek11}.

  We use Yao's minimax principle \cite{Yao}
  and give a distribution of inputs
  such that no deterministic algorithm gives better than a $3/4$-approximation
  in expectation.
  Let $X$, $Y$, and $Z$ be disjoint sets of $n$ variables.
  All of our inputs have the same set of clauses, all of weight 1:
  an equivalence of each pair of variables in $X \times Z$
  and an inequivalence of each pair of variables in $Y \times Z$.
  Each (in)equivalence is 2 clauses, so this is $4n^2$ clauses in total,
  $2n^2$ of which are always satisfied
  and $2n^2$ of which depend on satisfying the (in)equivalences.
  The randomized input has the $2n$ data items for variables in $X$ and $Y$
  in a uniformly random order,
  followed by the $n$ data items for variables in $Z$
  in some arbitrary fixed order.

  The key is that the data item for a variable has no information about
  the signs of other variables in its clauses,
  so variables in $X \cup Y$ are indistinguishable each other
  because each of them is in exactly 2 clauses with each variable in $Z$,
  once positive and once negative.
  This means a deterministic algorithm will always assign
  the same sequence of 0s and 1s
  to the variables of the first $2n$ data items.
  Suppose the algorithm assigns fraction $p$ of the variables to 0.
  Two distinct variables in $X$ are assigned different values
  with probability
  \[
    p\bigg(1-\frac{np-1}{n-1}\bigg) + (1-p)\frac{np-1}{n-1}
    = 2p(1-p) \pm O\bigg(\frac{1}{n}\bigg),
  \]
  and each such pair contributes to an unsatisfied clause
  because we cannot have $b = z = 1-b$.
  Similarly, a variable in $X$ and a variable in $Y$
  are assigned the same value with probability
  \[
    p\frac{np-1}{n-1} + (1-p)\bigg(1-\frac{np-1}{n-1}\bigg)
    = 1 - 2p(1-p) \pm O\bigg(\frac{1}{n}\bigg),
  \]
  and each such pair contributes to an unsatisfied clause
  because we cannot have $b = z \neq b$.
  Therefore, the expected number of unsatisfied clauses
  is at least $n^2 - O(n)$,
  which, for any $\epsilon > 0$,
  is a $1/4 - \epsilon$ fraction of all $4n^2$ clauses
  for sufficiently large $n$.
\end{proof}

The proof of Theorem \ref{theorem:greedyDeterministic} is in similar style.
We give a sketch here and refer the reader
to \cite{PoloczekVideo} and \cite{DBLP:conf/esa/Poloczek11} for more details.

Given a deterministic adaptive greedy algorithm,
we will adversarially construct an input for which it has
a low approximation ratio.
The construction uses $n$ variables and proceeds in two phases,
during the first of which $G \approx n/5$ variables will be assigned.
We distinguish between these $G$ phase I variables
and the remaining $n-G$ phase II variables.
The clause set consists of an (in)equivalence between every pair of variables
and some one-variable clauses,
all of which have equal weight.
We can delay the decisions of whether a particular (in)equivalence
is an equivalence or inequivalence
and of exactly which one-variable clauses we will include.
We have to be careful to make sure that
the adaptive algorithm can't force us to reveal the data item
of a phase II variable while phase I is still happening,
but we don't concern ourselves with these details for this sketch.

At any stage of the algorithm, every phase II variable $x$
has a ``score'' $(a(x),b(x))$
defined as follows:
if we think of reducing a clauses using the values of assigned variables,
e.g.\ reducing $x \lor y$ to $y$ if $x = 0$,
then $a(x)$ and $b(x)$ are
the numbers of $\ol{x}$ and $x$ clauses, respectively.
We eventually have to assign $x$,
at which point at least $\min\{a(x),b(x)\}$ clauses will be unsatisfied.
We supply one-variable clauses at the start to give each phase II variable
a starting score of either $(0,G)$ or $(G,0)$.

During phase I,
each phase I variable $x$ is assigned to either 0 or 1
by the deterministic algorithm.
In response to that assignment,
we determine the (in)equivalence between $x$ and each phase II variable $y$
such that neither $a(y)$ nor $b(y)$ exceeds $G$.
For example, if $x$ is assigned 0 and $y$ has score $(0,G)$,
we would make an equivalence between $x$ and $y$,
because that equivalence consists of $\ol{x} \lor y$,
which is satisfied by $x = 0$,
and $x \lor \ol{y}$, which becomes $\ol{y}$ when $x = 0$,
increasing $a(y)$ to 1.
This means that every phase II variable has score $(G,G)$
at the end of phase I.
In phase II, we decide (in)equivalences in response to assignments
such that $a(x)$ and $b(x)$ never differ by more than 1
for any phase II variable $x$,
which maximizes the minimum number of clauses
each assignment will unsatisfy.
It remains to be shown that the optimal assignment
is sufficiently better than the assignment given by the algorithm,
that we can treat phase I and phase II variables so differently
without the adaptivity of the algorithm getting in the way,
and that we can delay the decisions we delay without contradicting ourselves.
As mentioned earlier,
\cite{PoloczekVideo} and \cite{DBLP:conf/esa/Poloczek11}
address these details.


\bibliography{18-416-FP.bib}
\bibliographystyle{plain}
\end{document}
	% line of code telling latex that your document is ending. If you leave this out, you'll get an error
