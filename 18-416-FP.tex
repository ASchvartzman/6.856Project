%Jennifer Pan, August 2011

\documentclass[11pt,letter]{article}
	% basic article document class
	% use percent signs to make comments to yourself -- they will not show up.
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{complexity}
\usepackage{amssymb}
\usepackage{amsthm}
% This gave Ziv a compile error: \usepackage[inputenc]{utf8}
\usepackage[charter]{mathdesign} % Ziv likes this font....
\usepackage[english]{babel}

	% packages that allow mathematical formatting

\usepackage{graphicx}
	% package that allowxs you to include graphics

\usepackage{setspace}
	% package that allows you to change spacing

\usepackage{fullpage}
	% package that specifies normal margins

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\numberwithin{theorem}{section}

% Make life easier and use nicer looking greek letters, etc.
\newcommand{\ol}{\overline}
\renewcommand{\phi}{\varphi}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\emptyset}{\varnothing}


\begin{document}
	% line of code telling latex that your document is beginning

\title{On some recent purely combinatorial 3/4-approximation algorithms for the Maximum Satisfiability Problem}

\author{Jonathan Elzur, Ariel Schvartzman, Ziv Scully \\
  \{\texttt{jelzur}, \texttt{arielsc}, \texttt{ziv}\}\texttt{@mit.edu}}
% Note: when you omit this command, the current dateis automatically included

\maketitle
	% tells latex to follow your header (e.g., title, author) commands.
\begin{abstract}
A number of randomized $\frac{3}{4}$-approximation algorithms for MAX SAT have been developed in recent years. These algorithms sequentially randomly assign values of true or false to each variable based on currently unsatisfied clauses. The first such algorithm, developed by Poloczek and Schnitger \cite{Poloczek:2011:RVJ:2133036.2133087} works by carefully choosing the probability of assignment for each variable. Van Zuylen's algorithm \cite{vanZuylen:2011:SAM:2238496.2238512} uses a potential function to greatly simply calculations. We show the relationship between these algorithms, how they developed from one another, and discuss the limits of their approach.
\end{abstract}

\section{Introduction}

In the Maximum Satisfiability Problem (MAX SAT) we are given a set of boolean formula in conjunctive normal, with
variables $x_1, ..., x_n$ and clauses $C_1,...,C_m$. We want to find an
assignment of the variables that satisfies the most clauses.
In class we analyzed the unweighted case, but here we consider non-negative weights $w_i$ for the clauses
and ask for the weight of the satisfied clauses to be maximized. This problem, which is a generalization of the
Boolean Satisfiability Problem (SAT), is also $\NP$-hard.

This problem has been studied extensively. Let $W = \sum w_i$ be the sum of the clause weights.
A simple randomized algorithm sets each variable to true with probability $1/2$ and hence,
by linearity of expectation, satisfies $(1/2)W$ of the clauses. Johnson \cite{Johnson1974256}
gave the first deterministic algorithm to match this ratio. Consider rescaling the weight of a clause $C_j$
by its length in the following way: $f(C_j) = w_j 2^{-|C_j|}$. This weight function favors small clauses
since they are easier to falsify. Johnson's algorithm sets the $i$-th variable to be true
if the modified weight of the clauses containing $x_i$ is greater than that of the clauses containing its negation.
It was later shown that Johnson's algorithm was in fact the derandomization of the first algorithm and that the
approximation ratio of the derandomized algorithm could be improved to $2/3$ \cite{Chen1999622}.

Later, Yanakakis \cite{Yannakakis1994475} and separately Goemans and Williamson \cite{Goemans94new3/4-approximation}
gave algorithms which achieved $3/4$-approximation ratios. In fact, we studied one of these in class.
These algorithms relied heavily on linear programming relaxations. The best known approximation algorithm due to Avidor, Berkovitch
and Zwick achieves an approximation ratio of 0.7968. The algorithm is a hybrid which runs multiple MAX SAT approximation algorithms
(including LP-based ones) and returns the best \cite{Avidor:2005:IAA:2105211.2105214}. They also present an algorithm which is conjectured to have a performance of 0.843.

In 1999, Williamson posed the question
of whether or not the same approximation ratio could be achieved without using Linear Programming. In 2011,
Poloczek and Schnitger \cite{Poloczek:2011:RVJ:2133036.2133087} answered this question in the affirmative
by exhibiting the first purely combinatorial algorithm for MAX SAT. Their approach is similar to that of Johnson
but they introduce slight weight modifications that guarantee that at least $3/4$ of the weighted clauses will be satisfied.
While this result was exciting, the probability modifications are complicated and depend on the previous decisions
of the algorithm. Shortly after, van Zuylen \cite{vanZuylen:2011:SAM:2238496.2238512} gave a simpler analysis of the algorithm.
Both algorithms use randomness but are greedy in some sense,
setting a variable true or false with probability based on
how many clauses would be satisfied either way.

This paper is organized as follows.
In Section \ref{S:idea}, we present some notation and highlight the common techniques of both papers
and show how they could provide a $3/4$-approximation.
In Section \ref{S:PS}, we present the main ideas of Poloczek and Schtinger's paper.
In Section \ref{S:vZ}, we present van Zuylen's simplified approach.
In Section \ref{S:limits}, we present results on the limits of
the greedy approach the two algorithms share,
showing that the previous algorithms are in some sense optimal
and, more strikingly,
that no better than a $0.729$-approxmation is possible without randomness.


\section{The General Idea}\label{S:idea}

Let the input variables be $x_1,...,x_n$ and the input clauses be $C_1,...,C_m$,
%because Elzur is an asshole%
each with associated non-negative weights $w_1,...,w_m$. Let $\sum_{i=1}^{m} w_i = W$.
We are interested in finding an assignment of the variables $x_i$ such that the clauses satisfied have weight at least $\frac{3}{4}W$.

Our algorithm will be sequentially deciding what value to assign to each variable. Let $\mathrm{SAT}(i)$ be the total weight of the clauses
satisfied by our partial assignment $x_1, ..., x_i$ and let $\mathrm{UNSAT}(i)$ be the total weight of the clauses that are falsified (i.e. unsatisfiable) by the current partial assignment.
Suppose we have set variables $x_1,...,x_{i-1}$ and are considering a value for variable $x_i$. Then $\mathrm{SAT}(i)-\mathrm{SAT}(i-1)$ is the weight
of the clauses that become satisfied by assigning $x_i$ and similarly $\mathrm{UNSAT}(i) - \mathrm{UNSAT}(i-1)$ is the weight of the clauses that become
unsatisfiable by assigning $x_i$.
Notice that $\mathrm{SAT}(n)$ is the total weight of the clauses satisfied by our algorithm and $\mathrm{UNSAT}(n) = W - \mathrm{SAT}(n)$ is the weight of clauses
unsatisfied by the algorithm. We define sat($i$) = SAT($i$) - SAT($i-1$), unsat($i$) = UNSAT($i$) - UNSAT($i-1$). If the following inequality holds for all $i$, then the it is easy to show we can achieve a $3/4$-approximation ratio:

\begin{equation}
\label{eq:1}
\mathrm{sat}(i) - 3\mathrm{unsat}(i) \geq 0 \forall i.
\end{equation}

Adding these inequalities over all $i$ we get that

\begin{equation*}
\begin{aligned}
\sum_{i = 1}^{m} \mathrm{sat}(i) - 3\mathrm{unsat}(i) & = \mathrm{SAT}(n) - 3\mathrm{UNSAT}(n) \\
& = 4\mathrm{SAT}(n) - 3W \\
& \geq 0
\end{aligned}
\end{equation*}

From this we get that our algorithm's approximation ratio, $\textrm{SAT}(n)$, is at least $\frac{3}{4}$ of the maximum possible weight $W$.

There might not always exist an assignment for $i$ such that the equation \ref{eq:1} above holds. However, we only need it to hold on aggregate.
This is why the notion of a potential function $\phi$ is introduced. Throughout the algorithm, we will use the potential function to help us pay
for future mistakes where the inequality above might not hold.

Van Zuylen explains the desired behavior of this potential function in terms of other parameters of the problem. If these conditions are met, the algorithm
will achieve its guarantees. Let $\phi(i)$ be the value of the potential function after assigning the $i$-th variable. Then we need $\phi$ to satisfy the following properties:

\begin{itemize}
	\item $\phi(0) \leq 3(W-\textrm{OPT})$
	\item $\phi(n) \geq 0$
	\item For each $x_i$, the algorithm randomly determines a truth assignment to $x_i$ such that
\begin{equation}
\begin{aligned}
\label{eq:2}
\mathbb{E}[\mathrm{sat}(i) - 3\mathrm{unsat}(i)] \geq \mathbb{E}(\phi(i) - \phi(i-1)].
\end{aligned}
\end{equation}
\end{itemize}

If we have such a potential function, then $\mathbb{E}[\textrm{SAT}(n)- 3\left(W-\textrm{SAT}(n)\right)] \geq \phi(n) - \phi(0) \geq -3(W-\textrm{OPT})$.
This directly implies $\mathbb{E}[\textrm{SAT}(n)] \geq \frac{3}{4} \textrm{OPT}$.

We utilize the rest of this section to introduce some notation that will be useful for sections 3 and 4. Let $x_i^*$ be the optimal assignment of $x_i$ and
$x_i^{a}$ be the algorithm's assignment. We say a clause is alive
at time $i$ if it contains some literal $x_j$ with $j > i$ and it is not yet satisfied by $x_1^a,...,x_i^a$.
We will say a clause is contradictory at time $i$ if it is not satisfied by $x_1^a,...,x_i^a,x_{i+1}^*,...,x_n^*$.



\section{Poloczek and Schtinger's Approach}\label{S:PS}

In this section,
we will remind the reader about Johnson's Deterministic Algorithm. Motivated by the power of randomness, Poloczek and Schnitger
turn it into a randomized algorithm. Their first approach, the Canonical Randomization, can be shown to achieve an approximation ratio of at most $17/23$.
They overcome this barrier by introducing slight modifications to the probabilities and prove that there exist perturbations that yield a $3/4$ approximation.

\subsection*{Johnson's Deterministic Algorithm}

Consider, as mentioned in the introduction, the scaled clause weight.
For a clause $C_j$, with weight $w_j$, we have the modified weight:  $f(C_j) = w_j 2^{-|C_j|}$.

Johnson's algorithm processes the variables in an arbitrary order, and assigns them a value of 0 or 1 as follows. Let $x$ be the variable being assigned a value. We define the \emph{support} of $x_i$, $\mu_{x_i}$ to be the sum of the scaled weights of clauses containing $x_i$. Similarly, $\mu_{\bar{x_i}}$ is the sum of scaled weights of clauses containing $\bar{x_i}$, so

\begin{align*}
\mu_{x_i} = \sum_{C,x_i \in C}f(C), \,\,\,\, \mu_{\bar{x_i}} =\sum_{C,\bar{x_i}\in C}f(C)
\end{align*}
Johnson's algorithm greedily assigns $x_i =1$ if and only if $\mu_{x_1} \ge \mu_{\bar{x_i}}$. It is straightforward to show that Johnson's algorithm provides a $\frac{2}{3}$ approximation ratio. It can be shown also that Johnson's algorithm can provide a $\frac{2}{3} + c, c>0$ approximation ratio.

\subsection*{The Canonical Randomization}
Poloczek and Schnitger consider the \emph{Canonical Randomization} (CR) of Johnson's algorithm. Instead of deterministically assigning a value to the variable $x_i$, we assign it a value of 1 with probability proportional to the support of $x_i$ relative to that of $\bar{x_i}$. So
\[\Pr[\mathrm{assign }\, x_i=1] = \frac{\mu_{x_i}}{\mu_{x_i} + \mu_{\bar{x_i}}},\] and $x_i$ equals zero with the remaining probability. They prove the limits of this approach. We state their result without proof.

\begin{lemma}
\label{L:1}
The approximation ratio of CR algorithm is upper-bounded by
$$\frac{17}{23} < \frac{3}{4}.$$

\end{lemma}
The proof is constructive one. Poloczek and Schnitger construct a family of 2-CNF fomrulas which are fully satisfied by an assignment of all-ones, and show that as the number of variables gets large, the CR approximation approaches the above limit. This proof is quite long and out of this paper's scope.

\subsection*{Alternative Probabilities}
Due to Lemma \ref{L:1}, Poloczek and Schnitger decide to use an alternative weighting scheme. In this alternative, every clause with a single variable gets its weight doubled, while the remaining weights are unchanged. Therefore, the new support for $x$ which we can label $\lambda_{x_i}$ is
\[[\textrm{sum of weights of clauses with }x_i\textrm{ with }\ge 2\textrm{ variables}] + 2[\textrm{sum of weights of clause containing just }x_i].\]

If we call the first sum $\mathsf{fanin}$, and label the second sum $w_{x_i}$ and $\mathsf{fanout}$, $w_{\bar{x_i}}$ their counterparts for $\bar{x_i}$, then
\begin{align*}
\lambda_{x_i} &\equiv \textrm{Support for } x_i = \mathsf{fanin} + 2w_{x_i}, \\
\lambda_{\bar{x_i}} &\equiv \textrm{Support for } \bar{x_i} = \mathsf{fanout} + 2w_{\bar{x_i}}.
\end{align*}

Where $\mathsf{fanout}$ and $w_{\bar{x_i}}$ are respectively \[[\textrm{sum of weights of clauses with }\bar{x}_i\textrm{ with }\ge 2\textrm{ variables}] ,\,\,\, [\textrm{sum of weights of clause containing just }x_i].\]
Finally, we define $\Delta$ to be the sum of the supports, so $\Delta = \lambda_{x_i} + \lambda_{\bar{x_i}} = \mathsf{fanin} + 2w_{x_i} + \mathsf{fanout} + 2w_{\bar{x_i}}$. If we do a canonical randomization using the new weight functions, we see that
\begin{align*}
q_0 &:= \Pr[x_i=0] = \frac{\mathsf{fanout} + 2w_{\bar{x_i}}}{\Delta}, \\
q_1 &:= \Pr[x_i=1] = \frac{\mathsf{fanin} + 2w_{x_i}}{\Delta}.
\end{align*}
Note that for a $2-\text{CNF}$ formula, these alternative weights are exactly equivalent to the weights used in Johnson's algorithm. Thus, these modified weights still don't guarantee a $\frac{3}{4}$ approximation ratio.

\subsection*{Slack Algorithm}
The general approach of this algorithm is to change the probabilities, $q_0, q_1$ with which we assign values to $x_i$. In a sense, we're making the algorithm "less random" by increasing the higher of the two probabilities, bringing it closer to 1, and decreasing the lower of the two probabilities.

Assume w.l.o.g that $q_0 \le q_1$, so $x$ is at least as likely to assigned a value of 1. We now define the modified probabilities: $p_1 = q_1 + \varepsilon$, $p_0 = q_0 - \varepsilon$. We will show that for a good choice of $\varepsilon$, these probabilities yield a $\frac{3}{4}$ approximation. Doing this requires proving a few lemmas which Poloczek and Schnitger do in their paper. In order to better show the logical progression, we will defer some of the proofs to the end.

For any variable $x_i$, we define the random variable $\mathsf{slack}$ to be the magnitude of the difference between the support of $x_i$ and that of $\bar{x_i}$:
\begin{equation}
\mathsf{slack} = |(\mathsf{fanout} + 2w_{\bar{x_i}}) -(\mathsf{fanin} + 2w_{x_i})|.
\end{equation}
The main idea of the algorithm is that the magnitude of $\varepsilon$, which is the adjustment to the probabilities, will depend on the $\mathsf{slack}$. Intuitively, if $\mathsf{slack} = 0$ exactly, then we expect $\varepsilon = 0$, because there is no reason to change the probability of assigning either zero or 1. Similarly, suppose $\mathsf{slack}$ is very large, and there is zero support for $x_i = 0$. Then, again we expect $\varepsilon = 0$, because there is no need to change $q_0 = 0$. We only want $\varepsilon$ to be sizable for medium sized slack. Thus, it is reasonable to guess that $\varepsilon$ is negatively quadratic in we expect $\mathsf{slack}$. We will formally verify this intuition.
\begin{theorem}
Choosing $$\varepsilon = \varepsilon_1 \equiv \frac{-\frac{\mathsf{slack}^2}{\Delta} + \frac{\mathsf{slack}}{\Delta}(w_{x_i} + w_{\bar{x_i}})}{2\mathsf{slack}+ \mathsf{fanout} + \mathsf{fanin}}$$ yields probabilities that give a $\frac{3}{4}$ approximation.
\end{theorem}
We derive this result algebraically. Let $x_i$ be the $i^{th}$ variable fixed. Following our notation in the introduction, let SAT($i$), UNSAT($i$) be random variables that denote the weight of all clauses that are satisfied, and unsatisfiable once we fix $x_i$, respectively. Recall sat($i$) = SAT($i$) - SAT($i-1$), unsat($i$) = UNSAT($i$) - UNSAT($i-1$). It follows that
\begin{align}
\mathbb{E}[\textrm{sat}(i)] &= p_0(\mathsf{fanout} + 2w_{\bar{x_i}}) + p_1(\mathsf{fanin} + 2w_{x_i})\\
\mathbb{E}[\textrm{unsat}(i)] &= p_0w_{x_i} + p_1w_{\bar{x_i}}
\end{align}
Now, we call a clause, $C$ containing $x_i$ or its negation \textit{alive} iff after assigning a value to $x_i$, $C$ still has not been satisfied or falsified. Further, we define the random variable ``wounded" to be the sum of the weights of all alive clauses. Therefore
\begin{equation}
\mathbb{E}[\textrm{wounded}] = p_0 \cdot \mathsf{fanin} + p_1 \cdot \mathsf{fanout}
\end{equation}
Now Poloczek and Schnitger prove the following Lemma:
\begin{lemma} \label{L:2}
$\mathbb{E}[\textrm{sat}(i) - 3\cdot\textrm{unsat}(i)] = \mathbb{E}[\textrm{wounded} + \mathsf{slack}] - (w_{x_i} + w_{\bar{x_i}})$
\end{lemma}
Let us consider some optimal assignment, $\pi$, which has assignments $x_1^*, x_2^* \ldots x_n^*$. Let us define $F_i$ to be the weight of the clauses which we called \emph{contradictory}. That is, the weight of alive clauses that are not satisfied by $\pi$ after assigning a value to $x_i$. Similarly, $F_{i-1}$ is the weight of all contradictory clauses  after we assign a value to $x_{i-1}$. By linearity of expectations, we can add the quantity $-2(F_{i}-F_{i-1})$ to both sides of the equation of lemma \ref{L:2}, yielding:
\begin{equation}
\mathbb{E}[\textrm{sat}(i) - 3\cdot\textrm{unsat}(i) - 2(F_{i}-F_{i-1})] = \mathbb{E}[\textrm{wounded} + \mathsf{slack} - 2(F_{i}-F_{i-1})] - (w_{x_i} + w_{\bar{x_i}})
\end{equation}
If we can manage to find $p_0,p_1$ so that the right side of the equation is always positive, then we have
\begin{equation}
\mathbb{E}[\textrm{sat}(i) - 3\cdot\textrm{unsat}(i)+ 2(F_{i-1}-F_i)] \ge 0 \label{C:1}
\end{equation}
So the algorithm will have a $\frac{3}{4}$ approximation ratio if equation \ref{C:1} is satisfied. It is worth noting at this
point that the term $\mathbb{E}(F_{i}-F_{i-1})$ is equal to
$\mathbb{E}(\phi(i) - \phi(i-1))$ mentioned in the previous section. To achieve the approximation ratio, the next lemma is useful:
\begin{lemma} \label{L:3}
Assume our assignment probabilities for $x_i$ are $\Pr[x_i = 1] = p_1, \Pr[x_i =0] = p_0$. If the optimal assignment is $x_i =1$, then
\[\mathbb{E}[\textrm{wounded} - 2(F_{i}-F_{i-1})] - (w_{x_i} - w_{\bar{x_i}}) \ge \frac{\mathsf{slack}}{\Delta}(w_{x_i} - w_{\bar{x_i}}) + \varepsilon(\mathsf{fanin} + \mathsf{fanout}).\]
If the optimal assignment is  $x_i=0$, then
\[\mathbb{E}[\textrm{wounded} - 2(F_{i}-F_{i-1})] - (w_{x_i} - w_{\bar{x_i}}) \ge \frac{\mathsf{slack}}{\Delta}(w_{x_i} - w_{\bar{x_i}}) - \varepsilon(\mathsf{fanin} + \mathsf{fanout}).\]
\end{lemma}
The proof of this lemma involves quite a bit of algebra and is therefore omitted here. The consequence of this lemma, though, is that we can use it to find a value for $\varepsilon$ so that equation \ref{C:1} is always satisfied.

We want to express this constraint in terms of slack. So we note that from our definition of slack:
\begin{align*}
\mathbb{E}[\mathsf{slack}] &= (p_1-p_0)\cdot ((\mathsf{fanout} + 2w_{\bar{x_i}}) -(\mathsf{fanin} + 2w_{x_i}))
\\ &= (q_1-q_0+2\epsilon)(q_1-q_0)\cdot\Delta
\\ &= \Delta[(q_1-q_0)^2 + 2\varepsilon(q_1-q_0)]
\\ &= \frac{\mathsf{slack}^2}{\Delta} +2\varepsilon\cdot\mathsf{slack}.
\end{align*}
Plugging this into equation \ref{C:1}, and applying lemma \ref{L:3}, we get that if $x_i =1$ is the optimal assignment,
\[\mathbb{E}[\textrm{sat}(i) - 3\cdot\textrm{unsat}(i)+ 2(F_{i}-F_{i-1})] \ge \frac{\mathsf{slack}^2}{\Delta} +2\varepsilon\cdot\mathsf{slack} - \frac{\mathsf{slack}}{\Delta}(w_{x_i} + w_{\bar{x_i}}) + \varepsilon(\mathsf{fanin} + \mathsf{fanout}).\]
Setting the this expression equal to zero and solving for $\varepsilon$ yields
\begin{equation}
\varepsilon = \varepsilon_1 \equiv \frac{-\frac{\mathsf{slack}^2}{\Delta} + \frac{\mathsf{slack}}{\Delta}(w_{x_i} + w_{\bar{x_i}})}{2\mathsf{slack}+ \mathsf{fanout} + \mathsf{fanin}}.
\end{equation}
Thus this value of $\varepsilon$ yields exactly zero for the expression we want to be nonnegative. It's easy to check this also works for the case when $x_i=0$ is an optimal assignment. So, for every variable, we calculate $q_1, q_0$, calculate $\varepsilon_1$, and use it to get $p_1, p_0$. Then we assign $x_i=1$ with probability $p_1$. As we've just shown, this yields $\mathbb{E}[SAT] \ge \frac{3}{4}\textrm{OPT}$. However, this approach uses a lot of algebra, much of which can be simplified by introducing a potential function, as we do in the next section.

\section{van Zuyeln's Simplified Algorithm}\label{S:vZ}

In this section we present van Zuyeln's simplified analysis of Poloczek and Schnitger's algorithm.

\begin{lemma}
Consider the algorithm that sequentially assigns variables $x_1,...,x_n$. Given the assignment of $x_1,...,x_{i-1}$,
 let $W_i, \overline{W_i}$ be the weight of the clauses that are not yet satisfied and contain $x_i, \overline{x_i}$ respectively,
 but do not contain $x_{i+1}, ..., x_n$. Let $F_i, \overline{F_i}$ be the total weight of the clauses that are not yet satisfied
 that contain $x_i, \overline{x_i}$ respectively. Let $\alpha = \frac{W_i + F_i - \overline{W_i}}{F_i + \overline{F_i}}$, and let $x_i$ be set to true with probability

\begin{displaymath}
  p = \left\{
     \begin{array}{lr}
       0 & : \alpha \leq 0\\
       \alpha & : \alpha \in (0,1) \\
       1 & : \alpha \geq 1.
     \end{array}
   \right.
\end{displaymath}
Then the expected weight of the clauses satisfied by the algorithm is at least $\frac{3}{4} \textrm{OPT}$.
\end{lemma}

\begin{proof}
We will present a potential function $\phi$ such that it meets the criterion presented in section \ref{S:idea}. We claim that the potential function
$\phi(i) = 2(F_i)$ works (the potential function corresponds to the $2(F_i - F_{i-1})$ term in Poloczek and Schnitger's equation \ref{C:1}).
Note that at time $0$,
 $\phi(0) = 2(W-\textrm{OPT})$. It is clear that the first two conditions will be met, so we will focus on showing equation \ref{eq:2}.
 For this purpose we will lower bound the weight of the contradictory clauses at time $i-1$ that are not alive at time $i$
 and upper bound the weight of the clauses that become contradictory at time $i$.

Note that a clause that is contradictory at time $i-1$ is no longer contradictory at time $i$ when either it becomes satisfied
or it contains literals $x_j$, $j > i$. Therefore a lower bound for the weight of contradictory clauses alive at time $i-1$ and
 not alive at time $i$ by $W_i \mathbbm{1}_{x_i^* = 0} + \overline{W_i} \mathbbm{1}_{x_i^* = 1}$, where $\mathbbm{1}_A$ is the indicator variable for event $A$ being true.

Similarly, the only clauses that can become contradictory at time $i$ are those that are still alive at time $i-1$ and become unsatisfied
 by the asigning $x_i$ to contradict $x_i^*$. Therefore, we can upper bound the weight of clauses that become contradictory by
 $F_i \mathbbm{1}_{x_i^* = 1} (1-p) + \overline{F_i} \mathbbm{1}_{x_i^* = 0}p$.

Then we get an upper bound on the change in potential:

\begin{equation}
\label{eq:3}
\phi(i) - \phi(i-1) \leq 2(-W_i + \overline{F_i} p) \mathbbm{1}_{x_i^* = 0} + 2(-\overline{W_i} + F_i(1-p))\mathbbm{1}_{x_i^* = 1}.
\end{equation}

On the other hand,

\begin{equation}
\begin{aligned}
\label{eq:4}
& \textrm{SAT}(i) - \textrm{SAT}(i-1) - 3(\textrm{UNSAT}(i) - \textrm{UNSAT}(i-1)) \\
& = p(W_i + F_i - 3\overline{W_i}) + (1-p)(\overline{W_i} + \overline{F_i} - 3W_i).
\end{aligned}
\end{equation}

We will show that equation $(\ref{eq:4}) \geq \text{equation } (\ref{eq:3})$ by showing an upper bound $B$ on the right hand side of equation (\ref{eq:3}) such that equation $(\ref{eq:4}) \geq B \geq \text{equation } (\ref{eq:3})$.

Consider the case $\alpha \leq 0$, i.e $W_i + F_i \leq \overline{W_i}$. Then equation $(\ref{eq:3})$ becomes equal to

\begin{equation*}
\begin{aligned}
& -2W_i \mathbbm{1}_{x_i^* = 0} + 2(-\overline{W}_i + F_i) \mathbbm{1}_{x_i^* = 1} \\
& \leq  -2W_i \mathbbm{1}_{x_i^* = 0} - 2W_i \mathbbm{1}_{x_i^* = 0} = -2W_i.
\end{aligned}
\end{equation*}

Then the right hand side of equation (\ref{eq:4}) is at least $\overline{W_i} + \overline{F_i} - 3W_i + 2W_i =
\overline{W_i} + \overline{F_i} - W_i.$

But this quantity can't be negative, because if it were we would combine it with $W_i + F_i - \overline{W_i} \leq 0$ to obtain $F_i +
\overline{F_i} < 0$, a contradiction.

Consider now the case where $\alpha = 1$, i.e. $W_i + F_i -\overline{W_i} \geq \overline{F_i} + F_i$ or $W_i - \overline{F_i} \geq \overline{W_i}$.
We can notice that equation (\ref{eq:3}) $= 2(-W_i + \overline{F_i}) \leq -2\overline{W_i}$. On the other hand, the right hand side of  (\ref{eq:4})
is at least $W_i + F_i + 2\overline{W_i} -3\overline{W_i} = W_i + F_i - \overline{W_i}$ which is at least $0$ since otherwise we would get
that $F_i + \overline{F_i} < 0$, a contradiction.

We now proceed with the more general case. The analysis is similar to the previous cases and will be left as an exercise
for the reader to save space and prevent further cluttering.
%Note that van Zuylen's algorithm has a significantly simplified form. This is due to the introduction of the potential function. In the Slack algorithm, equation \ref{C:1} needed to be satisfied at every time $i$. The potential function lets us ammortize the cost somehow (not sure how?)
\end{proof}


\section{Limits of the Greedy Approach}\label{S:limits}

Poloczek showed in \cite{DBLP:conf/esa/Poloczek11} not only that
the randomized algorithms of the previous two sections
give the best possible approximation, namely $3/4$,
of any algorithm using the same greedy approach,
but also that the randomness was necessary for that performance.
We spend this section explaining these results,
mimicking much of Poloczek's notation and terminology.

Our first order of business is to make ``greedy approach'' precise.
To do this, we imagine the input as being of a series of \emph{data items},
each of which corresponds to a variable $x$.
A \emph{greedy algorithm} inspects the data items in some order
and irreversibly assigns the variable for each data item
before moving on to the next.
A data item for $x$ contains a list of the following information
for each undecided clause $c$ that $x$ appears in:
\begin{itemize}
\item
  the sign of $x$ in $c$ (whether $x$ or $\ol{x}$ is in $c$),
\item
  the weight of $c$,
\item
  a list of the other unassigned variables (without sign information) in $c$.
\end{itemize}
Data items depend on what has been assigned so far.
Namely, as the algorithm executes, we update $x$'s data item by
\begin{itemize}
\item
  removing decided clauses,
\item
  removing assigned variables from undecided clauses,
\item
  and merging undecided clauses with the same sign of $x$
  and same set of unassigned variables by adding their weights.
\end{itemize}
Note that, for instance, $(x \lor y)$ and $(x \lor \ol{y})$
would be merged for $x$'s data item but not for $y$'s.
The intention of data items is not
to be an even slightly practical representation of the problem.
Rather, they help make precise the sort of information an algorithm uses.

An \emph{online greedy algorithm},
such as those from Sections~\ref{S:PS}~and~\ref{S:vZ},
is given the data items in some adversarial order,
while an \emph{adaptive greedy algorithm}
can choose at each step the sort of data item it would like to see next
without being able to actually inspect every data item.
Specifically, at each step,
an adaptive greedy algorithm
gives an ordering $\prec$ on all possible data items
and is given the $\prec$-minimal among remaining data item
with ties broken adversarially.
We know that online greedy algorithms can achieve $3/4$-approximations,
but we might hope that cleverer choice of weights
(e.g. $\alpha$ in van Zuyeln's algorithm)
could yield a better approximation.
Alas, no such cleverness is possible.

\begin{theorem}\label{theorem:greedyRandomized}
  No randomized online greedy algorithm
  is better than a $3/4$-approximation in expectation.
\end{theorem}

A small example (see \cite{DBLP:conf/esa/Poloczek11}) shows that
deterministic online greedy algorithms cannot do better than $2/3$,
and even even if we allow them to be adaptive, we still can't reach $3/4$.

\begin{theorem}\label{theorem:greedyDeterministic}
  No deterministic adaptive greedy algorithm
  is better than a $\frac{\sqrt{33} + 3}{12}$-approximation.
\end{theorem}

For the proofs of both of these theorems,
we will reason at a level of abstraction higher than clauses.
An \emph{equivalence} between two variables $x$ and $y$
is the pair of clauses $x \lor \ol{y}$ and $\ol{x} \lor y$.
Similarly, an \emph{inequivalence} is
the pair $x \lor y$ and $\ol{x} \lor \ol{y}$.
If all clauses have weight 1,
an equivalence contributes weight 2 when $x = y$
and only weight 1 when $x \neq y$,
while an inequivalence contributes weight 2 when $x \neq y$
and only weight 1 when $x = y$.
We use the term ``quivalence''
to refer to either an equivalence or inequivalence.
Abusing terminology slightly, we say a quivalence is satisfied
when it contributes weight 2 rather than only weight 1.

\begin{proof}[Proof of Theorem~\ref{theorem:greedyRandomized}]
  The construction given here is original
  but uses the same idea as that in \cite{DBLP:conf/esa/Poloczek11}.

  We use Yao's minimax principle \cite{Yao}
  and give a distribution of inputs
  such that no deterministic algorithm gives better than a $3/4$-approximation
  in expectation.
  Let $X$, $Y$, and $Z$ be disjoint sets of $n$ variables.
  All of our inputs have the same set of clauses, all of weight 1:
  an equivalence of each pair of variables in $X \times Z$
  and an inequivalence of each pair of variables in $Y \times Z$.
  Each quivalence is 2 clauses, so this is $4n^2$ clauses in total,
  $2n^2$ of which are always satisfied
  and $2n^2$ of which depend on satisfying the quivalences.
  The randomized input has the $2n$ data items for variables in $X$ and $Y$
  in a uniformly random order,
  followed by the $n$ data items for variables in $Z$
  in some arbitrary fixed order.

  The key is that the data item for a variable has no information about
  the signs of other variables in its clauses,
  so variables in $X \cup Y$ are indistinguishable each other
  because each of them is in exactly 2 clauses with each variable in $Z$,
  once positive and once negative.
  This means a deterministic algorithm will always assign
  the same sequence of 0s and 1s
  to the variables of the first $2n$ data items.
  Suppose the algorithm assigns fraction $p$ of the variables to 0.
  Two distinct variables in $X$ are assigned different values
  with probability
  \[
    p\bigg(1-\frac{np-1}{n-1}\bigg) + (1-p)\frac{np-1}{n-1}
    = 2p(1-p) \pm O\bigg(\frac{1}{n}\bigg),
  \]
  and each such pair contributes to an unsatisfied clause
  because we cannot have $b = z = 1-b$.
  Similarly, a variable in $X$ and a variable in $Y$
  are assigned the same value with probability
  \[
    p\frac{np-1}{n-1} + (1-p)\bigg(1-\frac{np-1}{n-1}\bigg)
    = 1 - 2p(1-p) \pm O\bigg(\frac{1}{n}\bigg),
  \]
  and each such pair contributes to an unsatisfied clause
  because we cannot have $b = z \neq b$.
  Therefore, the expected number of unsatisfied clauses
  is at least $n^2 - O(n)$,
  which, for any $\epsilon > 0$,
  is a $1/4 - \epsilon$ fraction of all $4n^2$ clauses
  for sufficiently large $n$.
\end{proof}

The proof of Theorem~\ref{theorem:greedyDeterministic} is in similar style.
We were unable to find a completely satisfying
(pun only slightly intended)
explanation of the construction.
The remainder of this section is our interpretation of a combination of
\cite{PoloczekVideo} and \cite{DBLP:conf/esa/Poloczek11},
the former of which was particularly helpful.
It is quite possible that some of the details of our explanation
are different from Poloczek's original intention,
but we obtain the same result.

Given a deterministic adaptive greedy algorithm (hereafter ``the algorithm''),
we (hereafter ``the adversary'')
will construct an input for which it has a low approximation ratio.
The construction uses $n$ variables and proceeds in two phases,
during the first of which $G \approx n/5$ variables will be assigned.
We distinguish between these $G$ phase I variables
and the remaining $n-G$ phase II variables.
The clause set consists of an quivalence between every pair of variables
and some one-variable clauses,
also called \emph{unit clauses}.
All the clauses in quivalences have weight 1,
and the unit clauses have weights between $0$ and $G$, inclusive.
Whether each quivalence is an equivalence or an inequivalence
is decided by the adversary as the algorithm executes,
as is the weight of each unit clause.

At any stage of the algorithm, every variable $x$
has a ``score'' $(a,b)$,
where $a$ is the current weight of the unit clause $x$
and $b$ is the current weight of the unit clause $\ol{x}$.
We will sometimes refer to this as the score of $x$'s data item.
Note that the score changes as the algorithm executes
and is not the same as the starting weights of those unit clauses.
When the algorithm assigns another variable $y$
while $x$ is still unassigned,
the pair of clauses that is the quivalence between $x$ and $y$
become a satisfied clause and a unit clause,
increasing one of $a$ or $b$ by 1.
We eventually have to assign $x$,
at which point clauses of weight at least $\min\{a,b\}$
and at most $\max\{a,b\}$ will be satisfied.

The most slippery parts of the adversarial construction
are claims that the adversary can ensure certain conditions hold
at certain stages of the algorithm, so we address those first.
Recall that the data item for $x$ contains
for each undecided clause $c$ that $x$ appears in
the sign of $x$ in $c$, the weight of $c$,
and a list of the other unassigned variables in $c$.
Recall also that we merge identical clauses by adding their weights.
Our input clauses will consist of unit clauses
and exactly one quivalence between every pair of variables.
Because equivalences and inequivalences between unassigned variables
are indistinguishable due to lack of sign information,
the only distinguishing feature of a variable
is the weights of the unit clauses it appears in.
This proves the following lemma.

\begin{lemma}\label{lemma:score}
  The only control the algorithm has over the data item it receives
  is the score.
\end{lemma}

In particular, the algorithm can't distinguish between
unit clauses that have been present since the start
and unit clauses created later by one side of a quivalence being assigned.
We turn our attention now to showing that the adversary
can enforce certain constraints on the scores of variables in phase I.

\begin{lemma}\label{lemma:phaseI}
  During phase I, when $i \leq G$ variables have been assigned,
  the adversary can ensure
  \begin{itemize}
  \item
    that the scores of unassigned variables are bounded (pointwise)
    by $(i,i)$ below and $(G,G)$ above
  \item
    and that when a variable $x$ is assigned by the algorithm,
    the quivalences between $x$ and the other $i$ assigned variables
    can be chosen without restriction.
  \end{itemize}
\end{lemma}

\begin{proof}
  This follows by induction on $i$.
  For $i = 0$, the bounds are satisfied
  as long as the adversary doesn't include unit clauses
  with weight greater than $G$ in the formula
  and there are no quivalences to be chosen.
  For the inductive step, the following intuition can be helpful.
  Imagine the adversary maintains a pool of the possible data items
  that both don't contradict information revealed to the algorithm so far
  and don't violate the bounds.
  For instance, if a single variable $x$ has been assigned to 0,
  there would be two possible data items of score $(1,1)$:
  one with starting score $(1,0)$ and an equivalence with $x$
  and another with starting score $(0,1)$ and an inequivalence with $x$.

  Thanks to Lemma~\ref{lemma:score},
  when the algorithm asks for a data item with some score,
  the adversary can watch the algorithm assign the data item's variable
  and, based on that assignment,
  decide which of the data items with the same score
  that are still in the pool it should have been.
  Suppose we are assigning variable $x$.
  The adversary updates the pool of possible data items
  by first adding the two ``children'' of each data item in the pool,
  one each having an equivalence and inequivalence with $x$,
  and then pruning any data items that violate the bounds.
  For example, the descendants of a data item starting with score $(2,1)$
  that remain after two variables have been assigned to 0
  are of score $(3,2)$, with one equivalence and one inequivalences,
  and score $(2,3)$, with two equivalences.
  The data item of score $(4,1)$ with two inequivalences
  is pruned from the pool for having $b < 2$.

  Thus, the bounds come for free from pruning,
  so for the inductive step,
  we just need to show that for any score and choice of quivalences,
  the data item with that score and choice of quivalences
  was not pruned at any previous time step.
  (More precisely,
  we should none of the ``ancestors'' of that data item,
  given that a data item can have two possible children.)
  Suppose the claim holds for some $i < G$
  and let $x$ be the variable under inspection
  after $i$ assignments have been made.
  We want to show that, once $x$ is assigned,
  for every score $(a,b)$ bounded by $(i+1,i+1)$ below and $(G,G)$ above
  and every choice of quivalences with assigned variables (including $x$)
  there is a possible data item with that score and those quivalences.
  Suppose without loss of generality that $x$ was assigned to 1
  and that we want a data item with score $(a,b)$ equivalence with $x$.
  (The other cases are analogous.)
  By the inductive hypothesis,
  for any choice of quivalences with the other $i$ assigned variables,
  there is a possible data item with score $(a-1,b)$ and those quivalences
  because $a-1 \geq i$.
  Adding the equivalence with $x$
  contributes weight 1 to the positive unit clause because $x$ was assigned 1,
  giving a data item of score of $(a,b)$ with the desired quivalences.
\end{proof}

A corollary of this is that at the beginning of phase II,
all the possible data items have score $(G,G)$
and can have any quivalences we like with the $G$ assigned variables.
We don't need as much flexibility for quivalences in phase II.
It is most helpful to think of variables in phase II
as being assigned in \emph{consecutive pairs}
starting with the very first variable of phase II.
That is, the first and second variables assigned in phase II
make up the first consecutive pair,
the second and third variables make up the second consecutive pair, etc.

\begin{lemma}\label{lemma:phaseII}
  During phase II, when $G+i$ variables have been assigned,
  the adversary can ensure
  \begin{itemize}
  \item
    that the scores of unassigned variables
    are $(G+i/2,G+i/2)$ if $i$ is even
    and $(G+(i\pm1)/2,G+(i\mp1)/2)$ if $i$ is odd
  \item
    and that after another variable $x$ is assigned,
    \begin{itemize}
    \item
      the adversary can choose the quivalences
      between $x$ and phase I variables without restriction
    \item
      and, for each consecutive pair $(y,z)$ assigned so far in phase II,
      the adversary can choose between
      the two choices of the $x$-$y$ and $x$-$z$ quivalences
      that are satisfied by opposite choices for $x$.
    \end{itemize}
  \end{itemize}
\end{lemma}

For example, if $y$ and $z$ have opposite values,
we can choose to have their quivalences with $x$
either both be equivalences or both be inequivalences.
We omit the proof, which is mostly mechanical
after using the same key insights as the proof of Lemma~\ref{lemma:phaseI}.
Notice that we make no guarantee about the quivalence
between two members of a consecutive pair.
Armed with this lemma, we are finally ready to give the full construction.

\begin{proof}[Proof of Theorem~\ref{theorem:greedyDeterministic}]
  We first make a distinction between two ``parities'' of phase II variables.
  Call the first variable assigned in phase II ``odd'',
  the second ``even'', and so on.

  The adversary constructs the clause set such that
  \begin{itemize}
  \item
    all quivalences between two phase I variables are satisfied,
  \item
    all quivalences between
    a phase I variable and an even phase II variable are unsatisfied,
  \item
    all quivalences between
    a phase I variable and an odd phase II variable are satisfied,
  \item
    all quivalences between
    phase II variables of the same parity are satisfied,
  \item
    and all quivalences between
    phase II variables of different parities
    that are not in the same consecutive pair
    are unsatisfied.
  \end{itemize}
  That the adversary can achieve all of these follows from
  Lemmas~\ref{lemma:phaseI}~and~\ref{lemma:phaseII}.
  To calculate an upper bound on the weight of the algorithm's assignment,
  we sum the maximum possible $\max\{a,b\}$
  of all scores $(a,b)$ of the variables as they are assigned,
  plus an additional term for the clause of each quivalence
  that's immediately satisfied even if the quivalence as a whole is not.
  This comes out to the sum of
  \begin{itemize}
  \item
    $\binom{n}{2}$ guaranteed from quivalences,
  \item
    $G$ scores contributing at most $G$ each
    in phase I (by Lemma~\ref{lemma:phaseI}),
  \item
    and $n-G$ scores contributing scores ranging from $G$ to $G + (n-G)/2$,
    or about $G + (n-G)/4$ each on average,
    in phase II (by Lemma~\ref{lemma:phaseII}).
  \end{itemize}
  Letting $G = \alpha n$, this comes out to
  \[
    W_\textrm{algorithm} = \frac{n^2}{4}(3 + 2\alpha + \alpha^2) + O(n).
  \]

  A much better assignment is obtained by complementing
  the phase I and odd phase II variables.
  In particular, every quivalence is satisfied
  with the possible exception of those between consecutive pairs,
  so there are at least $2\binom{n}{2} - (n-G)$ satisfied clauses.
  A more careful examination of the unit clauses
  shows that each phase II variable satisfies a unit clause of weight $G$
  and that each phase I variable satisfies
  a unit clause of weight $G/2$ on average.
  Specifically, at the end of phase I,
  an even phase II variable $x$ assigned to 1 has score $(G,G)$
  and has unsatisfied quivalences with phase I variables.
  Because those quivalences are unsatisfied,
  they must all contribute to the weight of $\ol{x}$,
  which means $x$ had weight $G$ from the beginning.
  The same reasoning applies to other phase II variables,
  and a similar argument shows that a phase I variable assigned
  after $i$ others satisfies a unit clause of weight $i$.
  Summing all of this gives
  \[
    W_\textrm{better} = \frac{n^2}{4}(4 + 4\alpha - 2\alpha^2) + O(n).
  \]
  Simple calculus shows that, ignoring the $O(n)$ term,
  $W_\textrm{algorithm}/W_\textrm{better}$
  is minimized when $\alpha = \frac{\sqrt{33} - 5}{4}$,
  giving a minimal ratio of $\frac{\sqrt{33} + 3}{12} < 3/4$.
  This means that for every deterministic algorithm,
  there is some input for which it cannot give better than a
  $\frac{\sqrt{33} + 3}{12}$-approximation.
\end{proof}

\bibliography{18-416-FP.bib}
\bibliographystyle{plain}
\end{document}
	% line of code telling latex that your document is ending. If you leave this out, you'll get an error
